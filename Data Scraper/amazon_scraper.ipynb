{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "63eac0a4",
      "metadata": {
        "id": "63eac0a4"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fo_WlJUj8Xtq",
      "metadata": {
        "id": "fo_WlJUj8Xtq"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_title(soup):\n",
        "\n",
        "    try:\n",
        "        # Outer Tag Object\n",
        "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
        "\n",
        "        # Inner NavigatableString Object\n",
        "        title_value = title.text\n",
        "\n",
        "        # Title as a string value\n",
        "        title_string = title_value.strip()\n",
        "\n",
        "    except AttributeError:\n",
        "        title_string = \"\"\n",
        "\n",
        "    return title_string\n",
        "\n",
        "def get_imgurl(soup):\n",
        "    try:\n",
        "        imgurl = soup.find(\"img\", attrs={'id':'landingImage'}).get('src')\n",
        "    except AttributeError:\n",
        "        imgurl = \"\"\n",
        "    return imgurl\n",
        "\n",
        "def fetch_amazon_data(pages):\n",
        "    HEADERS = ({'User-Agent': '', 'Accept-Language': 'en-US, en;q=0.5'})\n",
        "    # just change the k = in below link for example for washing machine , just modify k = washing+machine so the modified link becomes https://www.amazon.com/s?k=washing+machine&ref=nb_sb_noss_2\n",
        "    base_url = \"https://www.amazon.com/s?k=table&ref=nb_sb_noss_2\"\n",
        "    links_list = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        URL = f\"{base_url}&page={page}\"\n",
        "        webpage = requests.get(URL, headers=HEADERS)\n",
        "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "        links = soup.find_all(\"a\", attrs={'class': 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
        "\n",
        "        for link in links:\n",
        "            links_list.append(link.get('href'))\n",
        "\n",
        "    d = {\"title\": [], \"linkn\": [], \"imgurl\": []}\n",
        "\n",
        "    for link in links_list:\n",
        "        new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
        "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
        "        d['title'].append(get_title(new_soup))\n",
        "        d['linkn'].append(\"https://www.amazon.com\" + link)\n",
        "        d['imgurl'].append(get_imgurl(new_soup))\n",
        "\n",
        "    amazon_df = pd.DataFrame.from_dict(d)\n",
        "    amazon_df['title'].replace('', np.nan, inplace=True)\n",
        "    amazon_df = amazon_df.dropna(subset=['title'])\n",
        "    # just name the file according to your category name i.e washingmachine.csv\n",
        "    amazon_df.to_csv(\"table.csv\", header=True, index=False)\n",
        "    return amazon_df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pages_to_scrape = 10  # Set the number of pages you want to scrape\n",
        "    p = fetch_amazon_data(pages_to_scrape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa25f12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# code for automation by category\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_title(soup):\n",
        "    try:\n",
        "        title = soup.find(\"span\", attrs={\"id\": 'productTitle'})\n",
        "        title_value = title.text.strip()\n",
        "    except AttributeError:\n",
        "        title_value = \"\"\n",
        "    return title_value\n",
        "\n",
        "def get_imgurl(soup):\n",
        "    try:\n",
        "        imgurl = soup.find(\"img\", attrs={'id': 'landingImage'}).get('src')\n",
        "    except AttributeError:\n",
        "        imgurl = \"\"\n",
        "    return imgurl\n",
        "\n",
        "def fetch_amazon_data(category):\n",
        "    HEADERS = {'User-Agent': '', 'Accept-Language': 'en-US, en;q=0.5'}\n",
        "    base_url = f\"https://www.amazon.com/s?k={category}&ref=nb_sb_noss_2\"\n",
        "    links_list = []\n",
        "\n",
        "    for page in range(1, pages_to_scrape + 1):\n",
        "        URL = f\"{base_url}&page={page}\"\n",
        "        webpage = requests.get(URL, headers=HEADERS)\n",
        "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
        "        links = soup.find_all(\"a\", class_='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
        "\n",
        "        for link in links:\n",
        "            links_list.append(link.get('href'))\n",
        "\n",
        "    data = {\"title\": [], \"linkn\": [], \"imgurl\": [], \"category\": []}\n",
        "\n",
        "    for link in links_list:\n",
        "        new_webpage = requests.get(\"https://www.amazon.com\" + link, headers=HEADERS)\n",
        "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
        "        data['title'].append(get_title(new_soup))\n",
        "        data['linkn'].append(\"https://www.amazon.com\" + link)\n",
        "        data['imgurl'].append(get_imgurl(new_soup))\n",
        "        data['category'].append(category)\n",
        "\n",
        "    amazon_df = pd.DataFrame(data)\n",
        "    amazon_df['title'].replace('', np.nan, inplace=True)\n",
        "    amazon_df.dropna(subset=['title'], inplace=True)\n",
        "    amazon_df.to_csv(f\"{category}.csv\", header=True, index=False)\n",
        "    \n",
        "    return amazon_df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    categories = [\"watch\", \"jacket\"]  # List of categories to scrape\n",
        "    pages_to_scrape = 10  # Number of pages to scrape per category\n",
        "    \n",
        "    for category in categories:\n",
        "        print(f\"Scraping data for category: {category}\")\n",
        "        fetch_amazon_data(category)\n",
        "        print(f\"Scraping for category {category} complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363a0be6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
